#**Ejercicio**: Construir pipeline ETL completo con manejo robusto de errores y logging

"""
Logging:Logging es un sistema para registrar lo que hace tu programa mientras se est√° ejecutando.
Es como un diario o bit√°cora donde se va escribiendo:
qu√© acciones realiza el programa,
qu√© errores ocurren,
cu√°nto tarda cada parte,
qu√© datos se procesaron,
si algo sali√≥ mal y d√≥nde,
si todo sali√≥ bien.
Sirve para saber exactamente qu√© pas√≥ durante la ejecuci√≥n del programa.


¬øQu√© problema soluciona?

Los pipelines ETL suelen fallar por:
Datos corruptos
Archivos faltantes
Conexiones a bases de datos
Tipos de datos incorrectos
Campos nulos
Errores inesperados
Si no tienes logging, es imposible saber exactamente d√≥nde fall√≥ el ETL ni qu√© ocurri√≥.


"""
#Configurar logging estructurado:
import logging
import pandas as pd
import sqlite3
import time
from pathlib import Path

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('etl_pipeline.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('etl_pipeline')

#--------------------------------------------------------------------------------


#Crear clase de pipeline robusto:
"""
¬øQu√© es esa clase RobustETLPipeline?
Es una versi√≥n avanzada de un pipeline ETL que incorpora:
1Ô∏è Manejo de errores profesional
2Ô∏è Reintentos autom√°ticos
3Ô∏è Transacciones seguras
4Ô∏è M√©tricas de ejecuci√≥n
5Ô∏è Logging detallado

En otras palabras: Es un ETL que no se cae f√°cilmente y que deja registro de todo lo que pas√≥.

----------------------------
Logging = registro
RobustETL = proceso que usa ese registro
"""

"""
¬øQu√© hace __init__? Es el constructor. Se ejecuta autom√°ticamente cuando creas un pipeline.
Al ejecutarse RobustETLPipeline, hace:

self.db_path = db_path                  # ruta de la base de datos
self.logger = logging.getLogger(...)    # logger para registrar eventos
self.metrics = {...}                    # m√©tricas del pipeline
"""

"""
¬øQu√© hace run_pipeline?
Es el "jefe de la operaci√≥n".
Orquesta TODO el ETL:
Extrae datos ‚Üí extract_with_retry()
Transforma ‚Üí transform_with_validation()
Carga ‚Üí load_with_transaction()
Reporta √©xito ‚Üí report_success()
Si algo falla ‚Üí report_failure()
Lo hace dentro de un try‚Ä¶except:
"""

"""
. ¬øQu√© hace extract_with_retry?

Esta parte es MUY importante y tambi√©n MUY √∫til.
Normalmente, la extracci√≥n puede fallar:
API ca√≠da
archivo bloqueado
red intermitente
servidor lento
Entonces este m√©todo intenta 3 veces antes de rendirse.
"""
class RobustETLPipeline:    
    def __init__(self, db_path='etl_database.db'):  # Es el constructor.Se ejecuta autom√°ticamente cuando creas un pipeline.
        self.db_path = db_path
        self.logger = logging.getLogger('etl_pipeline')
        self.metrics = {'processed': 0, 'errors': 0, 'start_time': None}
    
    def run_pipeline(self): #Es el "jefe de la operaci√≥n". Orquesta TODO el ETL
        self.metrics['start_time'] = pd.Timestamp.now()
        self.logger.info("=== INICIANDO PIPELINE ETL ROBUSTO ===")
        
        try:
            # Fase 1: Extracci√≥n con reintentos
            data = self.extract_with_retry()
            
            # Fase 2: Transformaci√≥n con validaciones
            transformed_data = self.transform_with_validation(data)
            
            # Fase 3: Carga con transacciones
            self.load_with_transaction(transformed_data)
            
            self.report_success()
            
        except Exception as e:
            self.report_failure(e)
            raise
    
    def extract_with_retry(self):
        """Extracci√≥n con estrategia de reintentos"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"Intento de extracci√≥n #{attempt + 1}")
                
                # Simular extracci√≥n (reemplazar con l√≥gica real)
                data = pd.DataFrame({
                    'id': range(1, 101),
                    'valor': [x * 1.1 for x in range(1, 101)],
                    'categoria': ['A', 'B', 'C'] * 33 + ['A']
                })
                
                self.logger.info(f"Extracci√≥n exitosa: {len(data)} registros")
                return data
                
            except Exception as e:
                self.logger.warning(f"Intento #{attempt + 1} fall√≥: {e}")
                if attempt == max_retries - 1:
                    raise e
                time.sleep(1)  # Esperar antes de reintentar

    """
    En resumen
    ‚úî __init__
    Prepara el pipeline (ruta DB, logger, m√©tricas).
    ‚úî run_pipeline
    Ejecuta TODAS las fases en orden y maneja errores globales.
    ‚úî extract_with_retry
    Intenta extraer datos hasta 3 veces.
    Es un m√©todo que previene fallos en la etapa de extracci√≥n.
    """
    #-----------------------------------------------------------------------------------------

    #Implementar transformaci√≥n con validaciones:
    """
    ¬øQu√© es transform_with_validation?
    Es un m√©todo (una funci√≥n dentro de la clase) que realiza:
    Validaciones
    Transformaciones
    Control de errores
    Logging detallado
    Representa la segunda fase del ETL: la T de Transform (Transformaci√≥n).
    """
    def transform_with_validation(self, data):
            """Transformaci√≥n con validaciones y logging detallado"""
            self.logger.info("Iniciando transformaci√≥n")
            original_count = len(data)
            # Informa en el archivo log que est√° comenzando la transformaci√≥n. Guarda cu√°ntos registros
            # ten√≠a el dataframe original (para comparar despu√©s).
            try:
                # Validaci√≥n 1: Datos no nulos
                if data.isnull().any().any():
                    null_counts = data.isnull().sum()
                    self.logger.warning(f"Valores nulos encontrados: {null_counts[null_counts > 0].to_dict()}")
                """
                Busca si hay alg√∫n valor nulo en el dataframe.
                ‚úî Si hay nulos, registra un WARNING (no detiene el pipeline).
                ‚úî Te dice cu√°ntos nulos tiene cada columna.
                """
                # Transformaci√≥n 1: Limpiar datos Elimina todas las filas con nulos. Es una decisi√≥n com√∫n en ETL cuando no quieres imputar datos.
                data_clean = data.dropna()
                
                # Transformaci√≥n 2: Crear nuevas columnas
                data_clean = data_clean.copy()  # Evitar SettingWithCopyWarning
                data_clean['valor_cuadrado'] = data_clean['valor'] ** 2
                data_clean['categoria_normalizada'] = data_clean['categoria'].str.upper()
                
                # Validaci√≥n 2: Resultados razonables, Evita cargar datos corruptos o imposibles en la base de datos.
                if (data_clean['valor_cuadrado'] < 0).any():
                    raise ValueError("Valores cuadrados negativos detectados")
                
                self.logger.info(f"Transformaci√≥n exitosa: {original_count} -> {len(data_clean)} registros")
                return data_clean
                
            except Exception as e:
                self.logger.error(f"Error en transformaci√≥n: {e}")
                raise
    #-----------------------------------------------------------------------------------------------

    #Implementar carga con transacciones:
    """
    ¬øQu√© es load_with_transaction?

    Es un m√©todo dentro de la clase RobustETLPipeline que se encarga de la CARGA (Load) del proceso ETL.

    Su objetivo es:

    Guardar datos transformados en una base de datos SQLite
    De forma segura, controlada y sin riesgo de corrupci√≥n
    Usando transacciones para poder hacer rollback si algo sale mal
    """
    """
    Una transacci√≥n es un bloque de operaciones que funcionan como un todo o nada:

    ‚úî Si todo sale bien ‚Üí COMMIT ‚Üí se guardan los cambios
    ‚ùå Si algo falla ‚Üí ROLLBACK ‚Üí se revierte todo

    As√≠ te aseguras de que la base quede siempre en un estado v√°lido.
    """
    def load_with_transaction(self, data):
            """Carga con soporte transaccional y rollback"""
            self.logger.info("Iniciando carga a base de datos")
            
            with sqlite3.connect(self.db_path) as conn: #Abrir conexi√≥n a SQLite
                try:
                    # Iniciar transacci√≥n 
                    conn.execute('BEGIN TRANSACTION')
                    
                    # Crear tabla si no existe
                    conn.execute('''
                        CREATE TABLE IF NOT EXISTS datos_transformados (
                            id INTEGER PRIMARY KEY,
                            valor REAL,
                            categoria TEXT,
                            valor_cuadrado REAL,
                            categoria_normalizada TEXT
                        )
                    ''')
                    
                    # Limpiar datos previos (estrategia replace)
                    conn.execute('DELETE FROM datos_transformados')
                    
                    # Insertar datos del df a la tabla
                    data.to_sql('datos_transformados', conn, index=False, if_exists='append')
                    
                    # Commit transacci√≥n, confirmar cambios, si no hay error se guardan los cambios
                    conn.commit()
                    
                    self.logger.info(f"Carga exitosa: {len(data)} registros insertados")
                    
                except Exception as e:
                    # Rollback autom√°tico por context manager si hay algun error dentro
                    self.logger.error(f"Error en carga, ejecutando rollback: {e}")
                    raise

    #-------------------------------------------------------------------------------------------------------

    #Implementar reporting y ejecutar pipeline:Estas funciones informan el resultado final del pipeline, usando logging y m√©tricas.
        #
    def report_success(self):
        """Reportar m√©tricas de √©xito"""
        duration = pd.Timestamp.now() - self.metrics['start_time']
        self.logger.info("=== PIPELINE ETL COMPLETADO EXITOSAMENTE ===")
        self.logger.info(f"Duraci√≥n total: {duration}")
        self.logger.info(f"Registros procesados: {self.metrics.get('processed', 0)}")
    
    def report_failure(self, error):
        """Reportar detalles de fallo"""
        duration = pd.Timestamp.now() - self.metrics['start_time']
        self.logger.error("=== PIPELINE ETL FALL√ì ===")
        self.logger.error(f"Duraci√≥n hasta fallo: {duration}")
        self.logger.error(f"Error: {error}")

# Ejecuci√≥n del pipeline
if __name__ == "__main__":
    pipeline = RobustETLPipeline()
    pipeline.run_pipeline()
    
    # Verificar resultados en la base de datos
    with sqlite3.connect('etl_database.db') as conn:
        result = pd.read_sql('SELECT COUNT(*) as registros FROM datos_transformados', conn)
        print(f"Registros en base de datos: {result.iloc[0,0]}")





"""
BEGIN ‚Üí inicia transacci√≥n
Hace operaciones
Si todo bien ‚Üí COMMIT
Si algo fall√≥ ‚Üí ROLLBACK
Esto es lo que usa el pipeline.
"""

"""
Resumen general ultimo bloque

üîπ Cierra el pipeline
Llama al proceso ETL completo
Notifica si sali√≥ bien o mal
Mide tiempos y cantidad de datos
üîπ Facilita monitoreo
Logs de √©xito y error quedan escritos
Permiten revisar la salud del pipeline
üîπ Valida la carga final
Verifica que los datos se guardaron efectivamente en SQLite
"""

"""
Todo este ejercicio funciona con un DataFrame ‚Äúde mentira‚Äù que viene dentro del m√©todo extract_with_retry() solo para practicar la estructura completa de un 
pipeline ETL real.
Pero en un ETL real, t√∫ NO vas a tener ese DataFrame inventad
Entonces‚Ä¶ ¬øqu√© pasa a futuro?
‚úî 1. S√≠, debes reemplazar el DataFrame interno por tus fuentes reale

A futuro (real)
Podr√≠a ser:
Desde un CSV:
data = pd.read_csv("mis_clientes.csv")
Desde una API:
data = requests.get(url).json()
data = pd.DataFrame(data)
Desde una base SQL:
data = pd.read_sql("SELECT * FROM clientes", conn)

No necesitas eliminar nada, pero s√≠ reemplazar la fuente
Tu pipeline ETL queda as√≠ para producci√≥n:
EXTRACT ‚Üí TRANSFORM ‚Üí LOAD
La parte que cambia es EXTRACT.
Puedes dejar todo el pipeline igual, solo cambias el m√©todo extract_with_retry().


üëâ El pipeline ETL existe justamente para limpiar, validar y transformar los datos autom√°ticamente.
üìå Entonces‚Ä¶ ¬øcu√°ndo deben estar limpios los datos?
‚ùå Antes del ETL NO deben estar limpios
El ETL recibe datos sucios, incompletos, inconsistentes‚Ä¶ y su trabajo es arreglarlos.
‚úÖ Despu√©s del ETL s√≠ deben quedar limpios

el ETL identifica los problemas‚Ä¶ pero no los repara autom√°ticamente, a menos que t√∫ programes esa l√≥gica de reparaci√≥n dentro del pipeline.

"""